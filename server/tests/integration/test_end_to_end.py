"""
Integration tests for end-to-end workflow execution.

Tests the complete flow from FastAPI endpoint â†’ LangGraph workflow â†’ Response.

Usage:
    # From server/ directory
    python -m pytest tests/integration/test_end_to_end.py -v -s

    # Or run directly
    python tests/integration/test_end_to_end.py

Prerequisites:
    - Docker services running: docker-compose up -d
    - Neo4j seeded with data: python tests/scripts/seed_neo4j.py
    - Ollama models pulled: ./tests/setup_ollama.sh
"""

import asyncio
import sys
import os
from pathlib import Path
import time

# Add server directory to path
server_dir = Path(__file__).parent.parent.parent
sys.path.insert(0, str(server_dir))

# Load environment variables from .env file BEFORE importing app modules
from dotenv import load_dotenv
env_path = server_dir / ".env"
load_dotenv(env_path)

# For host testing, set localhost URLs (these override Docker defaults)
# Only set if not already set (allows override via shell environment)
if not os.getenv("NEO4J_URI"):
    os.environ["NEO4J_URI"] = "bolt://localhost:7687"
if not os.getenv("OLLAMA_BASE_URL"):
    os.environ["OLLAMA_BASE_URL"] = "http://localhost:11434"

print(f"Loaded environment from: {env_path}")
print(f"NEO4J_URI = {os.getenv('NEO4J_URI')}")
print(f"OLLAMA_BASE_URL = {os.getenv('OLLAMA_BASE_URL')}")

from app.graph.workflow import execute_workflow
from app.graph.state import QueryComplexity, ValidationStatus
from app.core.neo4j import Neo4jDriver, execute_cypher_query


async def test_connection_prerequisites():
    """Test that all prerequisites are met before running tests."""
    print("\n" + "="*60)
    print("TEST 0: Prerequisites Check")
    print("="*60)

    checks_passed = True

    # Check 1: Ollama connection
    try:
        from app.core.llm_provider import get_llm_client
        client = get_llm_client()
        print("âœ“ Ollama client initialized")
    except Exception as e:
        print(f"âœ— Ollama connection failed: {e}")
        print("  Make sure Ollama is running: ollama serve")
        checks_passed = False

    # Check 2: Neo4j connection
    try:
        result = await execute_cypher_query("MATCH (a:Aquifer) RETURN count(a) as count")
        count = result[0]["count"] if result else 0
        print(f"âœ“ Neo4j connected ({count} aquifers)")

        if count == 0:
            print("  âš  Warning: No aquifers in database. Run: python tests/scripts/seed_neo4j.py")
    except Exception as e:
        print(f"âœ— Neo4j connection failed: {e}")
        print("  Make sure Neo4j is running: docker-compose up -d neo4j")
        checks_passed = False

    # Check 3: Environment variables
    required_vars = ["NEO4J_URI", "NEO4J_USER", "NEO4J_PASSWORD"]
    for var in required_vars:
        if os.getenv(var):
            print(f"âœ“ {var} set")
        else:
            print(f"âœ— {var} not set")
            checks_passed = False

    print()
    return checks_passed


async def test_simple_query():
    """Test 1: Simple query execution."""
    print("\n" + "="*60)
    print("TEST 1: Simple Query - List Aquifers")
    print("="*60)

    try:
        print("Query: 'List all aquifers in Brazil'")
        print("Expected: SIMPLE complexity, 1-2 sub-tasks, basic listing\n")

        start_time = time.perf_counter()

        final_state = await execute_workflow(
            user_query="List all aquifers in Brazil",
            session_id="test-simple-001",
            expert_mode=True
        )

        elapsed_time = (time.perf_counter() - start_time)

        # Check query plan
        query_plan = final_state.get("query_plan")
        if query_plan:
            print(f"âœ“ Query Plan Created")
            print(f"  - Complexity: {query_plan.complexity}")
            print(f"  - Sub-tasks: {len(query_plan.subtasks)}")
            print(f"  - Reasoning: {query_plan.reasoning[:100]}...")
        else:
            print("âœ— No query plan generated")
            return False

        # Check generated queries
        generated_queries = final_state.get("generated_queries", [])
        if generated_queries:
            print(f"\nâœ“ Cypher Queries Generated ({len(generated_queries)} queries)")
            for i, query in enumerate(generated_queries, 1):
                print(f"  Query {i}: {query.cypher[:60]}...")
        else:
            print("âœ— No Cypher queries generated")
            return False

        # Check validation results
        validation_results = final_state.get("validation_results", [])
        if validation_results:
            valid_count = sum(1 for v in validation_results if v.status == ValidationStatus.VALID)
            total_retries = final_state.get("total_retries", 0)

            print(f"\nâœ“ Validation Complete")
            print(f"  - Valid queries: {valid_count}/{len(validation_results)}")
            print(f"  - Total retries: {total_retries}")

            for i, vr in enumerate(validation_results, 1):
                print(f"  - Query {i}: {vr.status} ({vr.execution_time_ms:.1f}ms)")
                if vr.results:
                    print(f"    Results: {len(vr.results)} records")
        else:
            print("âœ— No validation results")
            return False

        # Check analysis report
        analysis_report = final_state.get("analysis_report")
        if analysis_report:
            print(f"\nâœ“ Analysis Report Generated")
            print(f"  - Summary: {analysis_report.summary[:100]}...")
            print(f"  - Insights: {len(analysis_report.insights)}")
            print(f"  - Recommendations: {len(analysis_report.recommendations)}")
        else:
            print("âœ— No analysis report generated")
            return False

        # Check final response
        final_response = final_state.get("final_response")
        if final_response:
            print(f"\nâœ“ Final Response Generated ({len(final_response)} characters)")
            print(f"\nFirst 300 characters:")
            print(f"{final_response[:300]}...")
        else:
            print("âœ— No final response")
            return False

        # Check execution trace (expert mode)
        execution_trace = final_state.get("execution_trace", [])
        if execution_trace:
            print(f"\nâœ“ Execution Trace Available (Expert Mode)")
            print(f"  - Steps: {len(execution_trace)}")
            for step in execution_trace:
                print(f"    â€¢ {step.agent}: {step.duration_ms:.0f}ms")

        # Performance check
        print(f"\nâœ“ Total Execution Time: {elapsed_time:.2f}s")
        if elapsed_time < 30:
            print("  âœ“ Performance: Acceptable (<30s)")
        else:
            print("  âš  Performance: Slow (>30s) - consider model optimization")

        return True

    except Exception as e:
        print(f"âœ— Error: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_compound_query():
    """Test 2: Compound query with comparisons."""
    print("\n" + "="*60)
    print("TEST 2: Compound Query - Comparison")
    print("="*60)

    try:
        print("Query: 'Compare aquifers in Amazon Basin vs Permian Basin'")
        print("Expected: COMPOUND complexity, 2-3 sub-tasks\n")

        start_time = time.perf_counter()

        final_state = await execute_workflow(
            user_query="Compare the top aquifers in Amazon Basin vs Permian Basin",
            session_id="test-compound-001",
            expert_mode=False  # Test without expert mode
        )

        elapsed_time = (time.perf_counter() - start_time)

        # Check query plan
        query_plan = final_state.get("query_plan")
        if query_plan:
            print(f"âœ“ Query Plan Created")
            print(f"  - Complexity: {query_plan.complexity}")
            print(f"  - Sub-tasks: {len(query_plan.subtasks)}")

            if query_plan.complexity in [QueryComplexity.COMPOUND, QueryComplexity.ANALYTICAL]:
                print(f"  âœ“ Correctly classified as {query_plan.complexity}")
            else:
                print(f"  âš  Classified as {query_plan.complexity} (expected COMPOUND/ANALYTICAL)")
        else:
            print("âœ— No query plan generated")
            return False

        # Check multiple queries generated
        generated_queries = final_state.get("generated_queries", [])
        if len(generated_queries) >= 2:
            print(f"\nâœ“ Multiple Queries Generated ({len(generated_queries)} queries)")
        else:
            print(f"âš  Only {len(generated_queries)} query generated (expected 2+)")

        # Check final response has comparison
        final_response = final_state.get("final_response", "")
        comparison_keywords = ["compare", "comparison", "vs", "versus", "difference", "both"]
        has_comparison = any(keyword in final_response.lower() for keyword in comparison_keywords)

        if has_comparison:
            print(f"âœ“ Response contains comparison analysis")
        else:
            print(f"âš  Response may not contain comparison")

        print(f"\nâœ“ Total Execution Time: {elapsed_time:.2f}s")

        return True

    except Exception as e:
        print(f"âœ— Error: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_analytical_query():
    """Test 3: Analytical query with aggregations."""
    print("\n" + "="*60)
    print("TEST 3: Analytical Query - Recommendations")
    print("="*60)

    try:
        print("Query: 'Recommend the best aquifers for CO2 storage'")
        print("Expected: ANALYTICAL complexity, prescriptive recommendations\n")

        start_time = time.perf_counter()

        final_state = await execute_workflow(
            user_query="Recommend the best aquifers for CO2 storage based on porosity and depth",
            session_id="test-analytical-001",
            expert_mode=True
        )

        elapsed_time = (time.perf_counter() - start_time)

        # Check complexity
        query_plan = final_state.get("query_plan")
        if query_plan and query_plan.complexity == QueryComplexity.ANALYTICAL:
            print(f"âœ“ Correctly classified as ANALYTICAL")
        elif query_plan:
            print(f"âš  Classified as {query_plan.complexity} (expected ANALYTICAL)")

        # Check analysis quality
        analysis_report = final_state.get("analysis_report")
        if analysis_report:
            print(f"\nâœ“ Analysis Report")
            print(f"  - Insights: {len(analysis_report.insights)}")
            print(f"  - Recommendations: {len(analysis_report.recommendations)}")

            # Check for prescriptive recommendations
            if analysis_report.recommendations:
                print(f"\n  Recommendation sample:")
                rec = analysis_report.recommendations[0]
                print(f"  â€¢ {rec.action}")
                print(f"    Rationale: {rec.rationale[:80]}...")
                print(f"    Priority: {rec.priority}")

            # Check visualization hints
            if hasattr(analysis_report, 'visualization_hints') and analysis_report.visualization_hints:
                # VisualizationHint objects have a 'type' attribute
                hint_types = [h.type if hasattr(h, 'type') else str(h) for h in analysis_report.visualization_hints]
                print(f"\n  âœ“ Visualization hints: {', '.join(hint_types)}")

        print(f"\nâœ“ Total Execution Time: {elapsed_time:.2f}s")

        return True

    except Exception as e:
        print(f"âœ— Error: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_self_healing():
    """Test 4: Validator self-healing capability."""
    print("\n" + "="*60)
    print("TEST 4: Self-Healing Validation")
    print("="*60)

    try:
        from app.agents.validator import validate_and_execute
        from app.core.llm_provider import get_llm_client
        from app.graph.state import CypherQuery, create_initial_state

        llm = get_llm_client()

        # Create a minimal state for testing
        state = create_initial_state(
            user_query="Test query for self-healing",
            session_id="test-healing-001"
        )

        # Intentionally broken query (wrong label name)
        broken_query = CypherQuery(
            subtask_id=1,
            cypher="MATCH (a:Aquifer123) RETURN a.name LIMIT 5",  # Wrong label
            explanation="Broken query for testing self-healing",
            parameters={},
            expected_columns=["a.name"]
        )

        print("Testing with intentionally broken query:")
        print(f"  {broken_query.cypher}")
        print()

        result = await validate_and_execute(broken_query, llm, state)

        print(f"Validation Result:")
        print(f"  - Status: {result.status}")
        print(f"  - Retry count: {result.retry_count}")

        if result.healed_query:
            print(f"  - Original: {result.original_query}")
            print(f"  - Healed: {result.healed_query}")
            print(f"  âœ“ Self-healing successful!")

        if result.status == ValidationStatus.VALID:
            print(f"  - Execution time: {result.execution_time_ms:.1f}ms")
            print(f"  - Results: {len(result.results)} records")
            print(f"  âœ“ Query executed successfully after healing")
            return True
        elif result.status == ValidationStatus.HEALED:
            print(f"  âœ“ Query was healed (may need more retries)")
            return True
        else:
            print(f"  âš  Query validation status: {result.status}")
            if result.error_message:
                print(f"  Error: {result.error_message}")
            # Self-healing attempted but didn't succeed - this is acceptable for testing
            return result.retry_count > 0  # Pass if at least one retry was attempted

    except Exception as e:
        print(f"âœ— Error: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_expert_mode_trace():
    """Test 5: Expert mode execution trace."""
    print("\n" + "="*60)
    print("TEST 5: Expert Mode Execution Trace")
    print("="*60)

    try:
        print("Running query with expert_mode=True\n")

        final_state = await execute_workflow(
            user_query="What is the deepest aquifer in the database?",
            session_id="test-expert-001",
            expert_mode=True
        )

        execution_trace = final_state.get("execution_trace", [])

        if not execution_trace:
            print("âœ— No execution trace generated")
            return False

        print(f"âœ“ Execution Trace Generated ({len(execution_trace)} steps)\n")

        # Expected agents
        expected_agents = ["planner", "cypher-specialist", "validator", "analyst"]
        trace_agents = [step.agent for step in execution_trace]

        for agent in expected_agents:
            if agent in trace_agents:
                step = next(s for s in execution_trace if s.agent == agent)
                print(f"  âœ“ {agent}:")
                print(f"    - Duration: {step.duration_ms:.0f}ms")
                # ExecutionTraceStep uses 'error' field, not 'status'
                status = "error" if step.error else "success"
                print(f"    - Status: {status}")
                if step.error:
                    print(f"    - Error: {step.error[:50]}...")
            else:
                print(f"  âœ— {agent}: not in trace")

        # Calculate total time
        total_time = sum(step.duration_ms for step in execution_trace)
        print(f"\nâœ“ Total traced time: {total_time:.0f}ms")

        # Check if trace is in final response (for API integration)
        final_response = final_state.get("final_response", "")
        if "Generated Cypher" in final_response or "Query" in final_response:
            print("âœ“ Expert mode details likely included in response")

        return True

    except Exception as e:
        print(f"âœ— Error: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_error_handling():
    """Test 6: Error handling for impossible queries."""
    print("\n" + "="*60)
    print("TEST 6: Error Handling")
    print("="*60)

    try:
        print("Query: 'Find all aquifers on Mars'")
        print("Expected: Graceful error handling\n")

        final_state = await execute_workflow(
            user_query="Find all aquifers on Mars",
            session_id="test-error-001",
            expert_mode=False
        )

        final_response = final_state.get("final_response", "")

        if final_response:
            print("âœ“ Response generated (no crash)")

            # Check for helpful error message
            error_keywords = ["no", "not", "unable", "couldn't", "found 0", "none"]
            has_error_indicator = any(keyword in final_response.lower() for keyword in error_keywords)

            if has_error_indicator:
                print("âœ“ Response indicates no results found")

            print(f"\nResponse preview:")
            print(f"{final_response[:200]}...")

            return True
        else:
            print("âœ— No response generated")
            return False

    except Exception as e:
        print(f"âœ— Error: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_performance_benchmark():
    """Test 7: Performance benchmark."""
    print("\n" + "="*60)
    print("TEST 7: Performance Benchmark")
    print("="*60)

    try:
        test_queries = [
            "List 5 aquifers",
            "What is the average porosity?",
            "Find high permeability aquifers"
        ]

        times = []

        for i, query in enumerate(test_queries, 1):
            print(f"\nQuery {i}: '{query}'")

            start_time = time.perf_counter()

            final_state = await execute_workflow(
                user_query=query,
                session_id=f"test-perf-{i:03d}",
                expert_mode=False
            )

            elapsed_time = (time.perf_counter() - start_time)
            times.append(elapsed_time)

            print(f"  Time: {elapsed_time:.2f}s")

            if final_state.get("final_response"):
                print(f"  âœ“ Response generated")

        # Statistics
        avg_time = sum(times) / len(times)
        max_time = max(times)
        min_time = min(times)

        print(f"\n{'='*40}")
        print(f"Performance Statistics:")
        print(f"  - Average: {avg_time:.2f}s")
        print(f"  - Min: {min_time:.2f}s")
        print(f"  - Max: {max_time:.2f}s")

        # Performance threshold check
        if avg_time < 20:
            print(f"  âœ“ Average performance: Excellent (<20s)")
        elif avg_time < 30:
            print(f"  âœ“ Average performance: Good (<30s)")
        else:
            print(f"  âš  Average performance: Slow (>30s)")

        return True

    except Exception as e:
        print(f"âœ— Error: {e}")
        import traceback
        traceback.print_exc()
        return False


async def main():
    """Run all integration tests."""
    # Set up logging to file
    from tests.conftest import setup_test_logging, teardown_test_logging
    log_file = setup_test_logging("test_end_to_end")

    print("\n" + "="*60)
    print("INTEGRATION TEST SUITE - END-TO-END WORKFLOW")
    print("="*60)
    print("\nTesting complete workflow from query â†’ response")

    # Check prerequisites
    if not await test_connection_prerequisites():
        print("\nâœ— Prerequisites not met. Please fix and try again.")
        return False

    results = []

    # Test 1: Simple query
    results.append(await test_simple_query())

    # Test 2: Compound query
    results.append(await test_compound_query())

    # Test 3: Analytical query
    results.append(await test_analytical_query())

    # Test 4: Self-healing
    results.append(await test_self_healing())

    # Test 5: Expert mode
    results.append(await test_expert_mode_trace())

    # Test 6: Error handling
    results.append(await test_error_handling())

    # Test 7: Performance
    results.append(await test_performance_benchmark())

    # Summary
    print("\n" + "="*60)
    print("TEST SUMMARY")
    print("="*60)
    passed = sum(results)
    total = len(results)
    print(f"Passed: {passed}/{total}")

    if passed == total:
        print("\nâœ“ All integration tests passed! Task 1.5 is complete.")
        print("\nðŸŽ‰ Phase 1: The Brain Refactor is COMPLETE!")
        print("\nNext steps:")
        print("  1. Test via API: curl -X POST http://localhost:8000/api/v2/chat/message")
        print("  2. Test via frontend: http://localhost:5173")
        print("  3. Proceed to Phase 2: The Expert Interface")
    else:
        print(f"\nâœ— {total - passed} test(s) failed.")
        print("\nTroubleshooting:")
        print("  - Check Ollama: ollama list")
        print("  - Check Neo4j: docker-compose logs neo4j")
        print("  - Check data: python tests/scripts/seed_neo4j.py")

    # Clean up logging
    teardown_test_logging(log_file, passed, total - passed)

    return passed == total


if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)
